{
  "name": "llm-server-desktop",
  "version": "1.0.0",
  "description": "Desktop wrapper for LLM Server UI",
  "main": "main.js",
  "scripts": {
    "client": "npm run dev --prefix frontend",
    "server": "./llama.cpp/build/bin/llama-server --port ${LLAMA_PORT:-8080} --metrics --models-dir \"./llama.cpp/models\" --models-config \"./models-config.json\"",
    "server:mlx-proxy": "node mlx-verify-proxy.js",
    "server:all": "concurrently \"npm run server\" \"npm run server:mlx-proxy\"",
    "desktop": "wait-on http://localhost:5173 && electron .",
    "start": "npm run client",
    "build": "npm run build:native && vite build --prefix frontend && electron-builder",
    "build:native": "cd native && npm install && npm run build && cd ../mlx && npm install && npm run build",
    "rebuild:native": "cd native && npm run build && cd ../mlx && npm run build",
    "build:mlx": "cd mlx && npm install && npm run build",
    "postinstall": "npm run build:native"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "devDependencies": {
    "concurrently": "^9.2.1",
    "electron": "^39.2.6",
    "electron-builder": "^24.13.3",
    "wait-on": "^9.0.3"
  },
  "build": {
    "appId": "com.daios.llm-server",
    "productName": "LLM Server",
    "mac": {
      "category": "public.app-category.developer-tools"
    },
    "linux": {
      "target": [
        "AppImage",
        "deb"
      ],
      "category": "Development"
    },
    "files": [
      "frontend/dist/**/*",
      "main.js",
      "preload.js",
      "package.json",
      "native/**/*"
    ],
    "extraResources": [
      {
        "from": "llama.cpp/build/bin/llama-server",
        "to": "bin/llama-server"
      }
    ]
  }
}
